{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b00e73-5a6a-4e7e-8c86-2c9ef3519ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import trimesh\n",
    "import pyvista as pv\n",
    "import pymeshfix as mf\n",
    "from pymeshfix._meshfix import PyTMesh\n",
    "\n",
    "\n",
    "import vedo\n",
    "from vedo import *\n",
    "\n",
    "import open3d as o3d\n",
    "from open3d import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e2cb7-509f-466a-b93b-9bf456ea1915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct Mesh\n",
    "def correct_mesh(iso, smooth=0, taubin=False):\n",
    "   # Extract vertices and faces\n",
    "    vertices = iso.vertices \n",
    "    faces = iso.cells\n",
    "    # Clean vertices and faces\n",
    "    vertices, faces = mf.clean_from_arrays(vertices, faces)\n",
    "\n",
    "    # Load PyTMesh\n",
    "    mfix = PyTMesh(False)  # False removes extra verbose output\n",
    "    # Create array\n",
    "    mfix.load_array(vertices, faces)\n",
    "    \n",
    "    # Fix mesh\n",
    "        # Fills all the holes having at at most 'nbe' boundary edges. If\n",
    "        # 'refine' is true, adds inner vertices to reproduce the sampling\n",
    "        # density of the surroundings. Returns number of holes patched.  If\n",
    "        # 'nbe' is 0 (default), all the holes are patched.\n",
    "    mfix.fill_small_boundaries(nbe=0, refine=True)\n",
    "    \n",
    "    # Return vertices and faces\n",
    "    vert, faces = mfix.return_arrays()\n",
    "    triangles = np.empty((faces.shape[0], 4), dtype=faces.dtype)\n",
    "    triangles[:, -3:] = faces\n",
    "    triangles[:, 0] = 3\n",
    "    # Create mesh\n",
    "    mesh = pv.PolyData(vert, triangles)\n",
    "    # Apply Smoothing if Assigned\n",
    "    if smooth > 0:\n",
    "        if taubin:\n",
    "            # Smooth mesh\n",
    "            mesh = mesh.smooth_taubin(n_iter=smooth, non_manifold_smoothing=True)\n",
    "        else:\n",
    "            mesh = mesh.smooth(n_iter=smooth)\n",
    "\n",
    "    return mesh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94a21b-6be7-4ee7-8e6f-c8e2fcf93497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 features derived from shape measurements\n",
    "def shape_measurments(pv_mesh, r=1.0): # set search radius [1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1,0.05]\n",
    "    # Load the point cloud and normals\n",
    "    point_cloud = np.asarray(pv_mesh.points)\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(point_cloud)\n",
    "    \n",
    "# Remove Outliers ### EXPERIMENTAL ###########################\n",
    "    # Downsample the Point Cloud with a Voxel of 0.01\n",
    "    voxel_down_pcd = pcd.voxel_down_sample(voxel_size=0.01)\n",
    "    # Statistical Oulier Removal\n",
    "    pcd, ind = voxel_down_pcd.remove_statistical_outlier(nb_neighbors=100,\n",
    "                                                        std_ratio=2.0)\n",
    "### END OF EXPERIMENTA ####################################### \n",
    "\n",
    "    pcd.compute_convex_hull()\n",
    "    pcd.estimate_normals()\n",
    "    pcd.orient_normals_consistent_tangent_plane(1)\n",
    "    \n",
    "    mesh_verts = np.asarray(pcd.points)\n",
    "    mesh_norms = np.asarray(pcd.normals)\n",
    "        \n",
    "    # #  Transform the point cloud to the a PointCloud() object used by open3d        \n",
    "    pcd.points = utility.Vector3dVector(mesh_verts)\n",
    "    pcd.normals = utility.Vector3dVector(mesh_norms)\n",
    "    \n",
    "    # Calculate the KDTree\n",
    "    pcd_tree = geometry.KDTreeFlann(pcd)\n",
    "    \n",
    "    allVerts = np.ones([len(mesh_verts),1])\n",
    "    \n",
    "    # how many scales will be used \n",
    "    numScales = 1\n",
    "    \n",
    "    # Place holder for set of features\n",
    "    feature_set = []\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    #  go through all the points\n",
    "    while i<len(mesh_verts):\n",
    "            \n",
    "            pointScaleFeatures = []\n",
    "            \n",
    "            for j in [r]: \n",
    "                \n",
    "                # Constant for Stability \n",
    "                constant = 0.0\n",
    "                #  for each radius area scale find neighbours\n",
    "                [k_small, idx_small, distances_small] = pcd_tree.search_radius_vector_3d(pcd.points[i], j) # radius search set radius\n",
    "            \n",
    "              \n",
    "                currNdx = np.array(idx_small)\n",
    "                #  if there are less than 2 neightbours just add 0s \n",
    "                if (len(currNdx) <=2):\n",
    "                    linearity= 0 + constant\n",
    "                    planarity=0 + constant\n",
    "                    sphericity=0 + constant\n",
    "                    omnivariance=0 + constant\n",
    "                    anisotropy=0 + constant\n",
    "                    eigenentropy=0 + constant\n",
    "                    sumOFEigs=0 + constant\n",
    "                    changeOfCurvature=0 + constant\n",
    "                    farthestDist =  distances_small[len(distances_small)-1]/j\n",
    "                    pointDensity = k_small/j\n",
    "                    heightStd=0 + constant\n",
    "                    heightMax=0 + constant\n",
    "                    \n",
    "                    shapeDist_curr = np.zeros(50)\n",
    "                #  if there are enouigh neighbours then continue with computation\n",
    "                else:\n",
    "                    \n",
    "                    #  get heighbourhood points and normals\n",
    "                    nearestNeighbors_normals = mesh_norms[currNdx]\n",
    "                    nearestNeighbors_verts = mesh_verts[currNdx]\n",
    "                    \n",
    "                    #  calculate the covariance matrix, and eigenvalues\n",
    "                    cov_mat = np.cov([nearestNeighbors_verts[:,0],nearestNeighbors_verts[:,1],nearestNeighbors_verts[:,2]])\n",
    "                    eig_val_cov, eig_vec_cov = np.linalg.eigh(cov_mat)\n",
    "                    idx = eig_val_cov.argsort()[::-1]  \n",
    "                    eig_val_cov = eig_val_cov[idx]\n",
    "                    \n",
    "                    #  calculate the first 12 features derived from shape measurements\n",
    "                    linearity = (eig_val_cov[0] - eig_val_cov[1])/eig_val_cov[0]\n",
    "                    planarity = (eig_val_cov[1] - eig_val_cov[2])/eig_val_cov[0]\n",
    "                    sphericity = eig_val_cov[2]/eig_val_cov[0]\n",
    "                    omnivariance =(eig_val_cov[0]*eig_val_cov[1]*eig_val_cov[2]) **(1./3.)\n",
    "                    anisotropy = (eig_val_cov[0] - eig_val_cov[2])/eig_val_cov[0]\n",
    "                    eigenentropy = -(( eig_val_cov[0] * np.log(eig_val_cov[0])) + ( eig_val_cov[1] * np.log(eig_val_cov[1])) + ( eig_val_cov[2] * np.log(eig_val_cov[2])))\n",
    "                    sumOFEigs = eig_val_cov[0]+ eig_val_cov[1]+eig_val_cov[2]\n",
    "                    changeOfCurvature = eig_val_cov[2]/(eig_val_cov[0]+ eig_val_cov[1]+eig_val_cov[2])\n",
    "                    \n",
    "                    farthestDist =  distances_small[len(distances_small)-1]/j\n",
    "                    pointDensity = k_small/j\n",
    "                    heightMax = np.abs(np.dot(nearestNeighbors_normals.mean(axis=0),nearestNeighbors_verts.T)).max() - np.abs(np.dot(nearestNeighbors_normals.mean(axis=0),nearestNeighbors_verts.T)).min()\n",
    "                    heightStd = np.abs(np.dot(nearestNeighbors_normals.mean(axis=0),nearestNeighbors_verts.T)).std()\n",
    "               \n",
    "                #  Added to the other features and a feature vector is created\n",
    "                pointScaleFeatures.extend([linearity,planarity,sphericity,omnivariance,anisotropy,eigenentropy,sumOFEigs,changeOfCurvature,farthestDist,pointDensity,heightMax,heightStd])\n",
    "            #  The feature vector for each point is checked for NaN values and then concatenated\n",
    "            where_are_NaNs = np.isnan(pointScaleFeatures)\n",
    "            pointScaleFeatures = np.array(pointScaleFeatures)\n",
    "            pointScaleFeatures[where_are_NaNs] = 0\n",
    "            pointScaleFeatures = pointScaleFeatures.tolist()\n",
    "            feature_set.append(pointScaleFeatures)\n",
    "            \n",
    "            i+=1\n",
    "    \n",
    "    # DataFrame of averaged features\n",
    "    col_names = ['linearity','planarity','sphericity','omnivariance','anisotropy','eigenentropy','sumOFEigs','changeOfCurvature','farthestDist','pointDensity','heightStd', 'heightMax']\n",
    "    feature_df =  pd.DataFrame(feature_set, columns = col_names) # TO GET ENTIRE DATA FOR PLOTTTING\n",
    "\n",
    "    # Get  values\n",
    "    final_df = feature_df.mean()\n",
    "        \n",
    "    return final_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093ad3ab-defb-4e1e-815a-61bc2232594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Mesh Features\n",
    "\n",
    "# Container for All Data\n",
    "all_data_list = []\n",
    "\n",
    "# Setup Parameters\n",
    "structure=[24,25] # flag surface of interest\n",
    "ndecim = 6000 # mesh granularity\n",
    "# Smooth Parameters\n",
    "lamb = 1.0\n",
    "itr = 5\n",
    "\n",
    "# Scans to Exclude\n",
    "corrupt_scan_id = [] # subject IDs to exclude\n",
    "\n",
    "# Directory to mine\n",
    "main_dir = '' # input path to directory of interest\n",
    "\n",
    "\n",
    "vent_size = [] # non processed mesh size\n",
    "\n",
    "# Starting Time\n",
    "program_starts = time.time()\n",
    "        \n",
    "# ID's directory\n",
    "id_dirs = os.listdir(main_dir)\n",
    "# Sub-directory\n",
    "sub_dirs = [os.path.join(main_dir, sub_id) for sub_id in id_dirs if not 'DS_Store' in sub_id and not 'Mesh_Params' in sub_id]\n",
    "for sub_dir in sub_dirs:\n",
    "                ses_dir = os.listdir(os.path.join(sub_dir))\n",
    "                for ses in ses_dir[:2]:\n",
    "                    if not 'DS_Store' in ses:\n",
    "                        scan_dir = os.listdir(os.path.join(sub_dir, ses))\n",
    "                        synthseg_files = [file for file in scan_dir if 'sub' in file] # cneuro\n",
    "                        for synthseg_file in synthseg_files:\n",
    "                            # Define source path\n",
    "                            src_path = os.path.join(sub_dir, ses, synthseg_file)\n",
    "                            # Path details\n",
    "                            file_path = src_path\n",
    "                            # Find a base name\n",
    "                            basename = os.path.basename(file_path)\n",
    "                            # Get ID\n",
    "                            sub_id = basename.split('_')[0] # cNeuro\n",
    "                            if sub_id not in corrupt_scan_id:\n",
    "                                \n",
    "                                    ### MESH PROCESSING ########################################################################### \n",
    "                                    # Load File\n",
    "                                    nifti_file = nib.load(file_path) # load file from the path\n",
    "                                    segmentation_array  = nifti_file.get_fdata() # convert nifti segmentation file into numpy array\n",
    "                                    # Filter brain structure of interest and set a scene for mesh\n",
    "                                    scene = np.where(\n",
    "                                                    (segmentation_array!=0) & # remove background\n",
    "                                                    np.isin(segmentation_array.astype(int), structure), # flag structure of interest\n",
    "                                                    1, 0) # assign values\n",
    "                                                     \n",
    "                                    # Create volume using Vedo surfnets and use dual approach to create isosurface     \n",
    "                                    # Extract volume\n",
    "                                    vol = Volume(scene.astype(int),spacing =nifti_file.header.get_zooms()) # Extract volume and correct strectching along all axis\n",
    "                                    # Create mesh from mask\n",
    "                                    iso = vol.isosurface_discrete([1,]) # discreate\n",
    "                                \n",
    "                                    ### CLEAN ###########################################################################\n",
    "                                    # Create Clean Mesh\n",
    "                                    mesh_obj = correct_mesh(iso) # smooth=s, taubin=taubin_state\n",
    "                                                                \n",
    "                                    # Pyvista Mesh\n",
    "                                    pv_mesh = pv.wrap(mesh_obj)\n",
    "                                    # Extract Faces\n",
    "                                    pv_faces = pv_mesh.faces\n",
    "                                    # Extract Vertices\n",
    "                                    pv_vertices = pv_mesh.points\n",
    "                                    # Back to Vedo Mesh\n",
    "                                    iso = vedo.Mesh([pv_vertices, pv_faces])\n",
    "                                    \n",
    "                                    ### SUBDIVIDE ###########################################################################\n",
    "                                    if pv_vertices.shape[0] < ndecim:\n",
    "                                        # Extract Vertices\n",
    "                                        vedo_vertices = iso.vertices \n",
    "                                        # Extract Faces\n",
    "                                        vedo_faces = iso.cells\n",
    "                                        # Subdivide Mesh\n",
    "                                        tri_vertices, tri_faces =  trimesh.remesh.subdivide_loop(vedo_vertices, np.array(vedo_faces).astype(int), iterations=1)\n",
    "                                        # Tri Mesh\n",
    "                                        tri_iso = trimesh.Trimesh(tri_vertices, tri_faces) \n",
    "                                        # Convert to Vedo Mesh\n",
    "                                        iso = vedo.trimesh2vedo(tri_iso)\n",
    "\n",
    "                                    ### SMOOTH ###########################################################################\n",
    "                                    # Correct Mesh\n",
    "                                    pv_mesh =  pv.wrap(correct_mesh(iso))\n",
    "                                    # Convert to Tri Mesh\n",
    "                                    pv_mesh_faces = pv_mesh.faces.reshape((pv_mesh.n_cells, 4))[:, 1:] \n",
    "                                    # Convert to Tri Mesh\n",
    "                                    tri_iso = trimesh.Trimesh(pv_mesh.points, pv_mesh_faces) \n",
    "                                    # Convert to Vedo Mesh\n",
    "                                    iso = vedo.trimesh2vedo(tri_iso)\n",
    "                                \n",
    "                                    # ### DECIMATE ###########################################################################\n",
    "                                    # Decimate Pro\n",
    "                                    iso = iso.decimate_pro(\n",
    "                                        \tn=ndecim,\n",
    "                                        \t# preserve_topology=False,\n",
    "                                        \t# preserve_boundaries=False,\n",
    "                                        \tsplitting=True\n",
    "                                            )\n",
    "                                    ### VEDO Smooth ###########################################################################\n",
    "\n",
    "                                    # Correct Mesh\n",
    "                                    pv_mesh =  pv.wrap(correct_mesh(iso))\n",
    "                                    # Convert to Tri Mesh\n",
    "                                    pv_mesh_faces = pv_mesh.faces.reshape((pv_mesh.n_cells, 4))[:, 1:] \n",
    "                                    # Convert to Tri Mesh\n",
    "                                    tri_mesh = trimesh.Trimesh(pv_mesh.points, pv_mesh_faces) \n",
    "                                    # Smooth Mesh\n",
    "                                    trimesh.smoothing.filter_laplacian(tri_mesh, lamb=lamb,  iterations=itr,)\n",
    "\n",
    "#### TEST \n",
    "                                    # Convert to Vedo Mesh\n",
    "                                    iso = vedo.trimesh2vedo(tri_mesh)\n",
    "                                    # Correct Mesh\n",
    "                                    pv_mesh =  pv.wrap(correct_mesh(iso))\n",
    "                                    # Convert to Tri Mesh\n",
    "                                    pv_mesh_faces = pv_mesh.faces.reshape((pv_mesh.n_cells, 4))[:, 1:] \n",
    "                                    # Convert to Tri Mesh\n",
    "                                    tri_mesh = trimesh.Trimesh(pv_mesh.points, pv_mesh_faces) \n",
    "\n",
    "                                \n",
    "                                    ### COMPUTE FEATURES ###########################################################################\n",
    "                                    # Register Setup Parameters\n",
    "                                    d = ndecim\n",
    "                                    setup = f\"s{lamb}_i{itr}_d{d}\"\n",
    "                                                                            \n",
    "                                    # Compute Curvature Features\n",
    "                                    # Tri mesh\n",
    "                                    IMC = tri_mesh.integral_mean_curvature # Integral Mean Curvature\n",
    "                                                                                                                \n",
    "                                    # Compute Global Features\n",
    "                                    # Tri mesh\n",
    "                                    gf_tri_mesh = tri_mesh\n",
    "                                    SA = gf_tri_mesh.area\n",
    "                                    V = gf_tri_mesh.volume\n",
    "                                    SAVR  = SA/V\n",
    "                                                                         \n",
    "                                    # Capture Asphericity\n",
    "                                    # Generate points\n",
    "                                    pts = pv_mesh.points\n",
    "                                    # Find the best fitting ellipsoid to the points\n",
    "                                    elli = pca_ellipsoid(pts, pvalue=0.95) #  https://vedo.embl.es/docs/vedo/pointcloud.html#pca_ellipsoid\n",
    "                                    AS = elli.asphericity()  # asphericity\n",
    "                                    ASE = elli.asphericity_error()  # error on asphericity\n",
    "                                                                        \n",
    "                                    # Capture Convexity and Geometry Features\n",
    "                                    # Tri mesh\n",
    "                                    cx_tri_mesh = tri_mesh\n",
    "                                    # Compute Shape volume to Convex Hull volume ratio\n",
    "                                    CxVR = V / cx_tri_mesh.convex_hull.volume # Volume devided by convex hull volume\n",
    "                                    # Compute Shape area to Convex Hull area ratio\n",
    "                                    CxSAR = SA / cx_tri_mesh.convex_hull.area\n",
    "                                    # Compute Convex Hull volume\n",
    "                                    CxV = cx_tri_mesh.convex_hull.volume\n",
    "                                    # Compute Convex Hull area\n",
    "                                    CxSA = cx_tri_mesh.convex_hull.area   \n",
    "                                    # Compute Convex Hull area to volume ratio\n",
    "                                    CxSAVR = CxSA / CxV\n",
    "                                \n",
    "                                    # Save Mesh\n",
    "                                    ses_path =  os.path.join(sub_dir, ses) # session dir path\n",
    "                                    mesh_path =  os.path.join(ses_path, f'{sub_id}_mesh.stl') # mesh destination path\n",
    "                                    pv_mesh.save(mesh_path)  \n",
    "                                                                        \n",
    "                                    # Features: ['linearity','planarity','sphericity','omnivariance','anisotropy','eigenentropy','sumOFEigs','changeOfCurvature','farthestDist','pointDensity','heightStd', 'heightMax']\n",
    "                                    shape_m = shape_measurments(pv_mesh)\n",
    "                                    L = shape_m['linearity']\n",
    "                                    P = shape_m['planarity']\n",
    "                                    SP = shape_m['sphericity']\n",
    "                                    O = shape_m['omnivariance']\n",
    "                                    AT = shape_m['anisotropy']\n",
    "                                    ET = shape_m['eigenentropy']\n",
    "                                    ES = shape_m['sumOFEigs']\n",
    "                                    CC = shape_m['changeOfCurvature']\n",
    "                                    FD = shape_m['farthestDist']\n",
    "                                    PD = shape_m['pointDensity']\n",
    "                                    Hmax = shape_m['heightMax']\n",
    "                                    Hsd = shape_m['heightStd']\n",
    "        \n",
    "                                                                    \n",
    "                                    # PyVista Curvature Mean\n",
    "                                    GGC = np.mean(pv_mesh.curvature('gaussian'))\n",
    "                                    GMC = np.mean(pv_mesh.curvature('mean'))\n",
    "                                    Cmax = np.mean(pv_mesh.curvature('maximum'))\n",
    "                                    Cmin = np.mean(pv_mesh.curvature('minimum'))\n",
    "                                                                            \n",
    "                                    # Create feature DataFrame\n",
    "                                    # Measurment Names\n",
    "                                    var_names = ['SA', 'V', 'SAVR',  'IMC', 'As', 'AsE', \n",
    "                                    'GC', 'MC', 'CMax', 'CMin', \n",
    "                                    'L', 'P', 'S', 'O', 'A', 'EE', 'SE', 'CC', 'PD', 'FD', 'HMax', 'HSD',  \n",
    "                                    'CHV', 'CHSA', 'CHSAVR', 'CHSAR', 'CHVR',]\n",
    "                                    # Measurments\n",
    "                                    measurments = [SA,V,SAVR,IMC,AS,ASE,\n",
    "                                                   GGC,GMC,Cmax,Cmin,\n",
    "                                                   L,P,SP,O,AT,ET,ES,CC,FD,PD,Hmax,Hsd,\n",
    "                                                   CxV, CxSA,CxSAVR,CxSAR,CxVR,]\n",
    "                                \n",
    "                                    col_names = ['MEASURMENT', 'VALUE']\n",
    "                                    measurment_DF = pd.DataFrame(zip(var_names, measurments), columns=col_names)\n",
    "                                    print(measurment_DF)\n",
    "                                    # Extract Feature Values\n",
    "                                    f_values = measurments\n",
    "                                    # Add ID\n",
    "                                    f_values.append(sub_id)\n",
    "                                    # Add Setup\n",
    "                                    f_values.append(setup)\n",
    "                                    # Add Row to All Data List \n",
    "                                    all_data_list.append(f_values)\n",
    "                                    # Time per Loop\n",
    "                                    now = time.time()\n",
    "                                    print(\"It took {} seconds to process {} setup {}\".format(now - program_starts,sub_id,setup))\n",
    "                                                                                           \n",
    "        \n",
    "# Create Final DataFrame \n",
    "all_data_cols = ['SA', 'V', 'SAVR',  'IMC', 'As', 'AsE', \n",
    "                    'GC', 'MC', 'CMax', 'CMin', \n",
    "                    'L', 'P', 'S', 'O', 'A', 'EE', 'SE', 'CC', 'PD', 'FD', 'HMax', 'HSD',  \n",
    "                    'CHV', 'CHSA', 'CHSAVR', 'CHSAR', 'CHVR',\n",
    "                     'ID', 'SETUP'] \n",
    "all_data_df = pd.DataFrame(all_data_list, columns = all_data_cols)\n",
    "# Date Params\n",
    "year = datetime.now().year\n",
    "month = datetime.now().month\n",
    "day = datetime.now().day\n",
    "feature_data_dst = os.path.join(main_dir, f'feature_df_{day}-{month}-{year}.csv') # feature data dst\n",
    "# Save Data\n",
    "all_data_df.to_csv(feature_data_dst)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (trimesh_env)",
   "language": "python",
   "name": "trimesh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
